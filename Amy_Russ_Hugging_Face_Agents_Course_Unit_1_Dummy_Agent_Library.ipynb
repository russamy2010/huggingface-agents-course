{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/russamy2010/huggingface-agents-course/blob/main/Amy_Russ_Hugging_Face_Agents_Course_Unit_1_Dummy_Agent_Library.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dummy Agent Library\n",
        "\n",
        "In this simple example, **we're going to code an Agent from scratch**.\n",
        "\n",
        "This notebook is part of the <a href=\"https://www.hf.co/learn/agents-course\">Agent Course</a>, a free Course from beginner to expert, where you learn to build Agents.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/communication/share.png\" alt=\"Agent Course\"/>"
      ],
      "metadata": {
        "id": "fr8fVR1J_SdU"
      },
      "id": "fr8fVR1J_SdU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec657731-ac7a-41dd-a0bb-cc661d00d714",
      "metadata": {
        "tags": [],
        "id": "ec657731-ac7a-41dd-a0bb-cc661d00d714"
      },
      "outputs": [],
      "source": [
        "!pip install -q huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Serverless API\n",
        "\n",
        "In the Hugging Face ecosystem there is a conveniant feature called *Serverless API* that allows you to **test some models**. This uses a shared enpoints between multiples users to compute some requests. This is convenient to test some models.\n",
        "\n",
        "To run this cell, **you need a token**, this can be set in the \"settings\" tab under \"secret\" in your Hugging Face Account.\n",
        "\n",
        "Make sure to call it \"HF_TOKEN\""
      ],
      "metadata": {
        "id": "8WOxyzcmAEfI"
      },
      "id": "8WOxyzcmAEfI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5af6ec14-bb7d-49a4-b911-0cf0ec084df5",
      "metadata": {
        "tags": [],
        "id": "5af6ec14-bb7d-49a4-b911-0cf0ec084df5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "os.environ[\"HF_TOKEN\"]=\"hf_xxxxxxxxxxx\"\n",
        "\n",
        "client = InferenceClient(\"meta-llama/Llama-3.2-3B-Instruct\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c918666c-48ed-4d6d-ab91-c6ec3892d858",
      "metadata": {
        "tags": [],
        "id": "c918666c-48ed-4d6d-ab91-c6ec3892d858"
      },
      "outputs": [],
      "source": [
        "# As seen in the LLM section, if we just do decoding, the model will only stop when it predicts an EOS token.\n",
        "# This does not happen here because we forgot to add the special tokens of this model.\n",
        "output = client.text_generation(\n",
        "    \"The capital of france is\",\n",
        "    max_new_tokens=100,\n",
        ")\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As seen in the LLM section, if we just do decoding, **the model will only stop when it predicts an EOS token** and this does not happen here because **we forgot to add the special tokens of this model**."
      ],
      "metadata": {
        "id": "w2C4arhyKAEk"
      },
      "id": "w2C4arhyKAEk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we now add the special tokens related to the <a href=\"https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct\">Llama-3.2-3B-Instruct model</a> that we're using, the behaviour changes and is now the expected EOS."
      ],
      "metadata": {
        "id": "T9-6h-eVAWrR"
      },
      "id": "T9-6h-eVAWrR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec0b95d7-8f6a-45fc-b477-c2f95153a001",
      "metadata": {
        "tags": [],
        "id": "ec0b95d7-8f6a-45fc-b477-c2f95153a001"
      },
      "outputs": [],
      "source": [
        "# If we now add the special tokens related to Llama3.2 model, the behaviour changes and is now the expected oen.\n",
        "prompt=\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "The capital of france is<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
        "output = client.text_generation(\n",
        "    prompt,\n",
        "    max_new_tokens=100,\n",
        ")\n",
        "\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manually adding special tokens is equivalent to using the \"chat\" method:\n"
      ],
      "metadata": {
        "id": "1uKapsiZAbH5"
      },
      "id": "1uKapsiZAbH5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb536eea-f316-4902-aabd-55710e6c4347",
      "metadata": {
        "tags": [],
        "id": "eb536eea-f316-4902-aabd-55710e6c4347"
      },
      "outputs": [],
      "source": [
        "#This is the equivalent to use the \"chat\" method that will correctly format the conversation for you.\n",
        "\n",
        "# The chat method is the RECOMMANDED method to use in order to ensure a smooth transition between model.\n",
        "# Since this notebook is only educationnal, we will keep using the \"text_generation\" method to understand the details.\n",
        "output = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"The capital of france is\"},\n",
        "    ],\n",
        "    stream=False,\n",
        "    max_tokens=1024,\n",
        ")\n",
        "\n",
        "print(output.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chat method is the RECOMMENDED method to use in order to ensure a **smooth transition between models but since this notebook is only educational**, we will keep using the \"text_generation\" method to understand the details.\n"
      ],
      "metadata": {
        "id": "jtQHk9HHAkb8"
      },
      "id": "jtQHk9HHAkb8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dummy Agent\n",
        "\n",
        "In the previous sections, we saw that the **core of an agent library is to append information in the system prompt**.\n",
        "\n",
        "This system prompt is a bit more complex than the one we saw earlier, but it already contains:\n",
        "\n",
        "1. **Information about the tools**\n",
        "2. **Cycle instructions** (Thought → Action → Observation)"
      ],
      "metadata": {
        "id": "wQ5FqBJuBUZp"
      },
      "id": "wQ5FqBJuBUZp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c66e9cb-2c14-47d4-a7a1-da826b7fc62d",
      "metadata": {
        "tags": [],
        "id": "2c66e9cb-2c14-47d4-a7a1-da826b7fc62d"
      },
      "outputs": [],
      "source": [
        "# This system prompt is a bit more complex and actually contains the function description already appended.\n",
        "# Here we suppose that the textual description of the tools have already been appended\n",
        "SYSTEM_PROMPT = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
        "\n",
        "get_weather: Get the current weather in a given location\n",
        "\n",
        "The way you use the tools is by specifying a json blob.\n",
        "Specifically, this json should have a `action` key (with the name of the tool to use) and a `action_input` key (with the input to the tool going here).\n",
        "\n",
        "The only values that should be in the \"action\" field are:\n",
        "get_weather: Get the current weather in a given location, args: {\"location\": {\"type\": \"string\"}}\n",
        "example use :\n",
        "```\n",
        "{{\n",
        "  \"action\": \"get_weather\",\n",
        "  \"action_input\": {\"location\": \"New York\"}\n",
        "}}\n",
        "\n",
        "ALWAYS use the following format:\n",
        "\n",
        "Question: the input question you must answer\n",
        "Thought: you should always think about one action to take. Only one action at a time in this format:\n",
        "Action:\n",
        "```\n",
        "$JSON_BLOB\n",
        "```\n",
        "Observation: the result of the action. This Observation is unique, complete, and the source of truth.\n",
        "... (this Thought/Action/Observation can repeat N times, you should take several steps when needed. The $JSON_BLOB must be formatted as markdown and only use a SINGLE action at a time.)\n",
        "\n",
        "You must always end your output with the following format:\n",
        "\n",
        "Thought: I now know the final answer\n",
        "Final Answer: the final answer to the original input question\n",
        "\n",
        "Now begin! Reminder to ALWAYS use the exact characters `Final Answer:` when you provide a definitive answer. \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we are running the \"text_generation\", we need to add the right special tokens."
      ],
      "metadata": {
        "id": "UoanEUqQAxzE"
      },
      "id": "UoanEUqQAxzE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78edbd65-d19b-42ef-8248-e01218470d28",
      "metadata": {
        "tags": [],
        "id": "78edbd65-d19b-42ef-8248-e01218470d28"
      },
      "outputs": [],
      "source": [
        "# Since we are running the \"text_generation\", we need to add the right special tokens.\n",
        "prompt=f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "{SYSTEM_PROMPT}\n",
        "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "What's the weither in London ?\n",
        "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is equivalent to the following code that happens inside the chat method :\n",
        "```\n",
        "messages=[\n",
        "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "    {\"role\": \"user\", \"content\": \"What's the weither in London ?\"},\n",
        "]\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
        "\n",
        "tokenizer.apply_chat_template(messages, tokenize=False,add_generation_prompt=True)\n",
        "```"
      ],
      "metadata": {
        "id": "L-HaWxinA0XX"
      },
      "id": "L-HaWxinA0XX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The prompt is now:"
      ],
      "metadata": {
        "id": "4jCyx4HZCIA8"
      },
      "id": "4jCyx4HZCIA8"
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt)"
      ],
      "metadata": {
        "id": "Vc4YEtqBCJDK"
      },
      "id": "Vc4YEtqBCJDK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s decode !"
      ],
      "metadata": {
        "id": "S6fosEhBCObv"
      },
      "id": "S6fosEhBCObv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2b268d0-18bd-4877-bbed-a6b31ed71bc7",
      "metadata": {
        "tags": [],
        "id": "e2b268d0-18bd-4877-bbed-a6b31ed71bc7"
      },
      "outputs": [],
      "source": [
        "# Do you see the problem ?\n",
        "output = client.text_generation(\n",
        "    prompt,\n",
        "    max_new_tokens=200,\n",
        ")\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do you see the problem ?\n",
        "The **answer was hallucinated by the model**. We need to stop to actually execute the function !"
      ],
      "metadata": {
        "id": "9NbUFRDECQ9N"
      },
      "id": "9NbUFRDECQ9N"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fc783f2-66ac-42cf-8a57-51788f81d436",
      "metadata": {
        "tags": [],
        "id": "9fc783f2-66ac-42cf-8a57-51788f81d436"
      },
      "outputs": [],
      "source": [
        "# The answer was hallucinated by the model. We need to stop to actually execute the function !\n",
        "output = client.text_generation(\n",
        "    prompt,\n",
        "    max_new_tokens=200,\n",
        "    stop=[\"Observation:\"] # Let's stop before any actual function is called\n",
        ")\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Much Better !\n",
        "Let's now create a **dummy get weither function**. In real situation you could call and API."
      ],
      "metadata": {
        "id": "yBKVfMIaK_R1"
      },
      "id": "yBKVfMIaK_R1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4756ab9e-e319-4ba1-8281-c7170aca199c",
      "metadata": {
        "tags": [],
        "id": "4756ab9e-e319-4ba1-8281-c7170aca199c"
      },
      "outputs": [],
      "source": [
        "#Dummy function\n",
        "def get_weather(location):\n",
        "    return f\"the weather in {location} is sunny with low temperatures. \\n\"\n",
        "\n",
        "get_weather('London')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's concatenate the base prompt, the completion untill function execution and the result of the function as an Observation and resume the generation."
      ],
      "metadata": {
        "id": "IHL3bqhYLGQ6"
      },
      "id": "IHL3bqhYLGQ6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f07196e8-4ff1-41f4-8b2f-99dd550c6b27",
      "metadata": {
        "tags": [],
        "id": "f07196e8-4ff1-41f4-8b2f-99dd550c6b27"
      },
      "outputs": [],
      "source": [
        "# Let's concatenate the base prompt, the completion untill function execution and the result of the function as an Observation\n",
        "new_prompt=prompt+output+get_weather('London')\n",
        "print(new_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the new prompt:"
      ],
      "metadata": {
        "id": "Cc7Jb8o3Lc_4"
      },
      "id": "Cc7Jb8o3Lc_4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d5c6697-24ee-426c-acd4-614fba95cf1f",
      "metadata": {
        "tags": [],
        "id": "0d5c6697-24ee-426c-acd4-614fba95cf1f"
      },
      "outputs": [],
      "source": [
        "final_output = client.text_generation(\n",
        "    new_prompt,\n",
        "    max_new_tokens=200,\n",
        ")\n",
        "\n",
        "print(final_output)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}